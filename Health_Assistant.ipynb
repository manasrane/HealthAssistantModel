{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dK5CRJwfa62Y",
        "outputId": "8bdfa1ea-8ead-4141-f9e8-893dc2770f48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-intel\n",
            "  Downloading tensorflow_intel-0.0.1-py3-none-any.whl.metadata (582 bytes)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting pyinstaller (from tensorflow-intel)\n",
            "  Downloading pyinstaller-6.14.1-py3-none-manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Collecting twine (from tensorflow-intel)\n",
            "  Downloading twine-6.1.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.11/dist-packages (from tensorflow-intel) (1.2.2.post1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build->tensorflow-intel) (1.2.0)\n",
            "Collecting altgraph (from pyinstaller->tensorflow-intel)\n",
            "  Downloading altgraph-0.17.4-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting pyinstaller-hooks-contrib>=2025.4 (from pyinstaller->tensorflow-intel)\n",
            "  Downloading pyinstaller_hooks_contrib-2025.5-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting readme-renderer>=35.0 (from twine->tensorflow-intel)\n",
            "  Downloading readme_renderer-44.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: requests-toolbelt!=0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from twine->tensorflow-intel) (1.0.0)\n",
            "Requirement already satisfied: keyring>=15.1 in /usr/local/lib/python3.11/dist-packages (from twine->tensorflow-intel) (25.6.0)\n",
            "Collecting rfc3986>=1.4.0 (from twine->tensorflow-intel)\n",
            "  Downloading rfc3986-2.0.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting id (from twine->tensorflow-intel)\n",
            "  Downloading id-1.5.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine->tensorflow-intel) (3.3.3)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine->tensorflow-intel) (0.9.0)\n",
            "Requirement already satisfied: importlib_metadata>=4.11.4 in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine->tensorflow-intel) (8.7.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine->tensorflow-intel) (3.4.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine->tensorflow-intel) (4.2.1)\n",
            "Requirement already satisfied: jaraco.context in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine->tensorflow-intel) (6.0.1)\n",
            "Collecting nh3>=0.2.14 (from readme-renderer>=35.0->twine->tensorflow-intel)\n",
            "  Downloading nh3-0.2.21-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: docutils>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from readme-renderer>=35.0->twine->tensorflow-intel) (0.21.2)\n",
            "Requirement already satisfied: Pygments>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from readme-renderer>=35.0->twine->tensorflow-intel) (2.19.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.11.4->keyring>=15.1->twine->tensorflow-intel) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.11/dist-packages (from SecretStorage>=3.2->keyring>=15.1->twine->tensorflow-intel) (43.0.3)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from jaraco.classes->keyring>=15.1->twine->tensorflow-intel) (10.7.0)\n",
            "Requirement already satisfied: backports.tarfile in /usr/local/lib/python3.11/dist-packages (from jaraco.context->keyring>=15.1->twine->tensorflow-intel) (1.2.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring>=15.1->twine->tensorflow-intel) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring>=15.1->twine->tensorflow-intel) (2.22)\n",
            "Downloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_intel-0.0.1-py3-none-any.whl (7.1 kB)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m816.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyinstaller-6.14.1-py3-none-manylinux2014_x86_64.whl (724 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.0/725.0 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading twine-6.1.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyinstaller_hooks_contrib-2025.5-py3-none-any.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.2/437.2 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading readme_renderer-44.0-py3-none-any.whl (13 kB)\n",
            "Downloading rfc3986-2.0.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading altgraph-0.17.4-py2.py3-none-any.whl (21 kB)\n",
            "Downloading id-1.5.0-py3-none-any.whl (13 kB)\n",
            "Downloading nh3-0.2.21-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.0/739.0 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: altgraph, rfc3986, pyinstaller-hooks-contrib, numpy, nh3, tensorboard, readme-renderer, pyinstaller, ml-dtypes, id, scikit-learn, tensorflow, twine, tensorflow-intel\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed altgraph-0.17.4 id-1.5.0 ml-dtypes-0.5.1 nh3-0.2.21 numpy-2.1.3 pyinstaller-6.14.1 pyinstaller-hooks-contrib-2025.5 readme-renderer-44.0 rfc3986-2.0.0 scikit-learn-1.7.0 tensorboard-2.19.0 tensorflow-2.19.0 tensorflow-intel-0.0.1 twine-6.1.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "96650d5fe2cc47b6844e2472b31bb8bb",
              "pip_warning": {
                "packages": [
                  "numpy",
                  "sklearn",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Uninstall existing libraries to ensure clean installation\n",
        "#!pip uninstall -y transformers tensorflow keras scikit-learn numpy\n",
        "\n",
        "# Install necessary libraries, allowing compatible versions to be installed\n",
        "!pip install transformers scikit-learn numpy tensorflow-intel tensorflow --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvISAk3Wj8-U",
        "outputId": "1035927a-8e79-4d29-e24f-bbed0530ccd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /kaggle/input/symptom2disease\n",
            "/kaggle/input/symptom2disease/Symptom2Disease.csv\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"niyarrbarman/symptom2disease\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kb3c19zWkYdG",
        "outputId": "12a0c914-cb36-42e3-e85d-b4bfeb871a79"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1200,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 86,\n        \"min\": 0,\n        \"max\": 299,\n        \"num_unique_values\": 300,\n        \"samples\": [\n          203,\n          266,\n          152\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"diagnosis\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"Pneumonia\",\n          \"Jaundice\",\n          \"Psoriasis\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"symptom\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1153,\n        \"samples\": [\n          \"I've had back pain, a cough that won't go away, and limb weakness. I've been experiencing neck pain, instability, and balance concerns.\",\n          \"Recently, I've been having problems using the restroom. Going is incredibly difficult, and doing so hurts. In addition, I've been experiencing some butt soreness, and my stools have been bloody. My anus has also been quite itching and aggravated.\",\n          \"I've been exhausted and experiencing nausea and itching. In addition, I've lost weight and have a temperature. My urine is dark, and my skin has turned yellow. Additionally, I've been having stomach pain.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-340d8d20-7d33-4db4-903e-e1a5a6da2327\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>symptom</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Psoriasis</td>\n",
              "      <td>I have been experiencing a skin rash on my arm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Psoriasis</td>\n",
              "      <td>My skin has been peeling, especially on my kne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Psoriasis</td>\n",
              "      <td>I have been experiencing joint pain in my fing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Psoriasis</td>\n",
              "      <td>There is a silver like dusting on my skin, esp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Psoriasis</td>\n",
              "      <td>My nails have small dents or pits in them, and...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-340d8d20-7d33-4db4-903e-e1a5a6da2327')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-340d8d20-7d33-4db4-903e-e1a5a6da2327 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-340d8d20-7d33-4db4-903e-e1a5a6da2327');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c9aabe25-48d1-4a4a-a2b5-2baab605b98b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c9aabe25-48d1-4a4a-a2b5-2baab605b98b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c9aabe25-48d1-4a4a-a2b5-2baab605b98b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   Unnamed: 0  diagnosis                                            symptom\n",
              "0           0  Psoriasis  I have been experiencing a skin rash on my arm...\n",
              "1           1  Psoriasis  My skin has been peeling, especially on my kne...\n",
              "2           2  Psoriasis  I have been experiencing joint pain in my fing...\n",
              "3           3  Psoriasis  There is a silver like dusting on my skin, esp...\n",
              "4           4  Psoriasis  My nails have small dents or pits in them, and..."
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "try:\n",
        "    df = pd.read_csv('/kaggle/input/symptom2disease/Symptom2Disease.csv', encoding='latin1', sep=',')\n",
        "except UnicodeDecodeError:\n",
        "    df = pd.read_csv('/kaggle/input/symptom2disease/Symptom2Disease.csv', encoding='cp1252', sep=',')\n",
        "df.rename(columns={'text': 'symptom', 'label': 'diagnosis'}, inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1e9K-jpyT6D"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "## Setting up the environment\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, TFBertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS6e2hMBX94C",
        "outputId": "ced305cb-5784-4132-c64c-8328f93dbc69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "\n",
            "Sample Data:\n",
            "   Unnamed: 0  diagnosis                                            symptom  \\\n",
            "0           0  Psoriasis  I have been experiencing a skin rash on my arm...   \n",
            "1           1  Psoriasis  My skin has been peeling, especially on my kne...   \n",
            "2           2  Psoriasis  I have been experiencing joint pain in my fing...   \n",
            "3           3  Psoriasis  There is a silver like dusting on my skin, esp...   \n",
            "4           4  Psoriasis  My nails have small dents or pits in them, and...   \n",
            "\n",
            "   label  \n",
            "0      0  \n",
            "1      0  \n",
            "2      0  \n",
            "3      0  \n",
            "4      0  \n",
            "\n",
            "Diagnosis to Label Mapping:\n",
            "{'Psoriasis': 0, 'Varicose Veins': 1, 'Typhoid': 2, 'Chicken pox': 3, 'Impetigo': 4, 'Dengue': 5, 'Fungal infection': 6, 'Common Cold': 7, 'Pneumonia': 8, 'Dimorphic Hemorrhoids': 9, 'Arthritis': 10, 'Acne': 11, 'Bronchial Asthma': 12, 'Hypertension': 13, 'Migraine': 14, 'Cervical spondylosis': 15, 'Jaundice': 16, 'Malaria': 17, 'urinary tract infection': 18, 'allergy': 19, 'gastroesophageal reflux disease': 20, 'drug reaction': 21, 'peptic ulcer disease': 22, 'diabetes': 23}\n"
          ]
        }
      ],
      "source": [
        "# Check TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "## Mock Data Generation (Replace with your actual dataset)\n",
        "import kagglehub\n",
        "\n",
        "\n",
        "# Create a dummy dataset of patient symptoms and corresponding diagnoses\n",
        "\"\"\"\n",
        "data = {\n",
        "    'symptom': [\n",
        "        \"I have a headache and a sore throat.\",\n",
        "        \"I feel very tired and have a fever.\",\n",
        "        \"My stomach hurts and I feel nauseous.\",\n",
        "        \"I have a persistent cough and difficulty breathing.\",\n",
        "        \"I have a rash on my skin and it itches.\",\n",
        "        \"I have joint pain and stiffness.\",\n",
        "        \"I have blurred vision and dizziness.\",\n",
        "        \"I have chest pain and shortness of breath.\",\n",
        "        \"I have difficulty sleeping and feel anxious.\",\n",
        "        \"I have a runny nose and sneezing.\"\n",
        "    ],\n",
        "    'diagnosis': [\n",
        "        \"Common Cold\",\n",
        "        \"Flu\",\n",
        "        \"Gastroenteritis\",\n",
        "        \"Bronchitis\",\n",
        "        \"Allergy\",\n",
        "        \"Arthritis\",\n",
        "        \"Migraine\",\n",
        "        \"Angina\",\n",
        "        \"Anxiety\",\n",
        "        \"Allergy\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\"\"\"\n",
        "# Map diagnoses to numerical labels\n",
        "unique_diagnoses = df['diagnosis'].unique().tolist()\n",
        "diagnosis_to_label = {diagnosis: i for i, diagnosis in enumerate(unique_diagnoses)}\n",
        "df['label'] = df['diagnosis'].map(diagnosis_to_label)\n",
        "num_labels = len(unique_diagnoses)\n",
        "\n",
        "print(\"\\nSample Data:\")\n",
        "print(df.head())\n",
        "print(\"\\nDiagnosis to Label Mapping:\")\n",
        "print(diagnosis_to_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXCd3niFp2ix",
        "outputId": "75ca3694-4fb7-4015-c732-53e2dd1483d2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
          ]
        }
      ],
      "source": [
        "## Data Preprocessing\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['symptom'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and encode the text data\n",
        "def encode_text(tokenizer, text_list, max_length=128):\n",
        "    return tokenizer(\n",
        "        text_list,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_token_type_ids=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "max_length = 128 # You can adjust this based on your data\n",
        "\n",
        "train_encodings = encode_text(tokenizer, X_train.tolist(), max_length=max_length)\n",
        "test_encodings = encode_text(tokenizer, X_test.tolist(), max_length=max_length)\n",
        "\n",
        "# Create TensorFlow Datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': train_encodings['input_ids'],\n",
        "        'token_type_ids': train_encodings['token_type_ids'],\n",
        "        'attention_mask': train_encodings['attention_mask']\n",
        "    },\n",
        "    tf.constant(y_train.values)\n",
        "))\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': test_encodings['input_ids'],\n",
        "        'token_type_ids': test_encodings['token_type_ids'],\n",
        "        'attention_mask': test_encodings['attention_mask']\n",
        "    },\n",
        "    tf.constant(y_test.values)\n",
        "))\n",
        "\n",
        "# Batch the datasets\n",
        "batch_size = 16 # You can adjust this\n",
        "train_dataset = train_dataset.shuffle(len(X_train)).batch(batch_size)\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXaZCipOp-2D",
        "outputId": "54680a2f-8770-4454-f726-f5ca0a8b0d3d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Summary:\n",
            "Model: \"tf_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  109482240 \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        multiple                  0 (unused)\n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  18456     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109500696 (417.71 MB)\n",
            "Trainable params: 109500696 (417.71 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "## Model Building\n",
        "# Load the pre-trained BERT model for sequence classification\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "print(\"\\nModel Summary:\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CvvpoFiqE_j",
        "outputId": "35fe942d-6bfc-4f43-be7c-ab081b066958"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "60/60 [==============================] - 1837s 29s/step - loss: 2.8679 - accuracy: 0.2156 - val_loss: 2.2515 - val_accuracy: 0.5750\n",
            "Epoch 2/3\n",
            "60/60 [==============================] - 1725s 29s/step - loss: 1.5529 - accuracy: 0.7885 - val_loss: 0.8834 - val_accuracy: 0.9292\n",
            "Epoch 3/3\n",
            "60/60 [==============================] - 1738s 29s/step - loss: 0.5939 - accuracy: 0.9708 - val_loss: 0.3484 - val_accuracy: 0.9708\n",
            "\n",
            "Training History:\n",
            "{'loss': [2.8678576946258545, 1.5528892278671265, 0.5939411520957947], 'accuracy': [0.21562500298023224, 0.7885416746139526, 0.9708333611488342], 'val_loss': [2.2514634132385254, 0.883381187915802, 0.3483802378177643], 'val_accuracy': [0.574999988079071, 0.9291666746139526, 0.9708333611488342]}\n",
            "15/15 [==============================] - 136s 9s/step - loss: 0.3484 - accuracy: 0.9708\n",
            "\n",
            "Test Loss: 0.3484\n",
            "Test Accuracy: 0.9708\n",
            "15/15 [==============================] - 146s 9s/step\n",
            "\n",
            "Classification Report:\n",
            "                                 precision    recall  f1-score   support\n",
            "\n",
            "                           Acne       1.00      1.00      1.00         7\n",
            "                      Arthritis       1.00      1.00      1.00        10\n",
            "               Bronchial Asthma       1.00      1.00      1.00        11\n",
            "           Cervical spondylosis       1.00      1.00      1.00         7\n",
            "                    Chicken pox       1.00      0.83      0.91        12\n",
            "                    Common Cold       1.00      1.00      1.00        12\n",
            "                         Dengue       0.86      1.00      0.92        12\n",
            "          Dimorphic Hemorrhoids       1.00      1.00      1.00         7\n",
            "               Fungal infection       1.00      1.00      1.00        13\n",
            "                   Hypertension       1.00      1.00      1.00        10\n",
            "                       Impetigo       1.00      1.00      1.00        11\n",
            "                       Jaundice       1.00      1.00      1.00        11\n",
            "                        Malaria       1.00      1.00      1.00        11\n",
            "                       Migraine       1.00      1.00      1.00        10\n",
            "                      Pneumonia       1.00      1.00      1.00        11\n",
            "                      Psoriasis       1.00      0.83      0.91         6\n",
            "                        Typhoid       1.00      1.00      1.00         9\n",
            "                 Varicose Veins       1.00      1.00      1.00        12\n",
            "                        allergy       0.86      1.00      0.92        12\n",
            "                       diabetes       1.00      1.00      1.00         8\n",
            "                  drug reaction       1.00      1.00      1.00         5\n",
            "gastroesophageal reflux disease       0.82      0.82      0.82        11\n",
            "           peptic ulcer disease       0.90      0.82      0.86        11\n",
            "        urinary tract infection       1.00      1.00      1.00        11\n",
            "\n",
            "                       accuracy                           0.97       240\n",
            "                      macro avg       0.98      0.97      0.97       240\n",
            "                   weighted avg       0.97      0.97      0.97       240\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Model Training\n",
        "# Train the model\n",
        "epochs = 3 # You can adjust this based on your data and resources\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=epochs,\n",
        "    validation_data=test_dataset\n",
        ")\n",
        "\n",
        "print(\"\\nTraining History:\")\n",
        "print(history.history)\n",
        "## Model Evaluation\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(f\"\\nTest Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Predict on the test set\n",
        "predictions = model.predict(test_dataset)\n",
        "predicted_labels = tf.argmax(predictions.logits, axis=1).numpy()\n",
        "\n",
        "# Convert numerical labels back to diagnoses\n",
        "label_to_diagnosis = {i: diagnosis for diagnosis, i in diagnosis_to_label.items()}\n",
        "predicted_diagnoses = [label_to_diagnosis[label] for label in predicted_labels]\n",
        "true_diagnoses = [label_to_diagnosis[label] for label in y_test.values]\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_diagnoses, predicted_diagnoses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXerPd5uqPQ4",
        "outputId": "cfb0eaad-dcf3-408b-f56b-00cad812d254"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n",
            "\n",
            "Symptom: 'I have a terrible headache and feel dizzy.'\n",
            "Predicted Diagnosis: drug reaction\n",
            "1/1 [==============================] - 1s 622ms/step\n",
            "\n",
            "Symptom: 'My nose is running and I keep sneezing.'\n",
            "Predicted Diagnosis: Common Cold\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at ./health_ai_agent_model were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ./health_ai_agent_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def predict_diagnosis(symptom_text, model, tokenizer, diagnosis_map, max_length=128):\n",
        "    \"\"\"Predicts the diagnosis for a given symptom text.\"\"\"\n",
        "    encoding = encode_text(tokenizer, [symptom_text], max_length=max_length)\n",
        "    input_dict = {\n",
        "        'input_ids': encoding['input_ids'],\n",
        "        'token_type_ids': encoding['token_type_ids'],\n",
        "        'attention_mask': encoding['attention_mask']\n",
        "    }\n",
        "    predictions = model.predict(input_dict)\n",
        "    predicted_label = tf.argmax(predictions.logits, axis=1).numpy()[0]\n",
        "    predicted_diagnosis = diagnosis_map[predicted_label]\n",
        "    return predicted_diagnosis\n",
        "\n",
        "## Using the Health AI Agent for Prediction\n",
        "# Example usage of the trained agent\n",
        "new_symptom = \"I have a terrible headache and feel dizzy.\"\n",
        "predicted = predict_diagnosis(new_symptom, model, tokenizer, label_to_diagnosis)\n",
        "print(f\"\\nSymptom: '{new_symptom}'\")\n",
        "print(f\"Predicted Diagnosis: {predicted}\")\n",
        "\n",
        "new_symptom_2 = \"My nose is running and I keep sneezing.\"\n",
        "predicted_2 = predict_diagnosis(new_symptom_2, model, tokenizer, label_to_diagnosis)\n",
        "print(f\"\\nSymptom: '{new_symptom_2}'\")\n",
        "print(f\"Predicted Diagnosis: {predicted_2}\")\n",
        "\n",
        "# Save the trained model (optional)\n",
        "model.save_pretrained('./health_ai_agent_model')\n",
        "\n",
        "# Load the model (optional)\n",
        "loaded_model = TFBertForSequenceClassification.from_pretrained('./health_ai_agent_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KsvrN0FQYYIu",
        "outputId": "a610314d-3f80-4e3e-86bc-40e617119993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60/60 [==============================] - 1830s 30s/step - loss: 0.2542 - accuracy: 0.9948 - val_loss: 0.1975 - val_accuracy: 0.9875\n",
            "Epoch 2/5\n",
            "60/60 [==============================] - 1819s 30s/step - loss: 0.1320 - accuracy: 1.0000 - val_loss: 0.1361 - val_accuracy: 0.9833\n",
            "Epoch 3/5\n",
            "60/60 [==============================] - 1801s 30s/step - loss: 0.0823 - accuracy: 1.0000 - val_loss: 0.1000 - val_accuracy: 0.9875\n",
            "Epoch 4/5\n",
            "60/60 [==============================] - 1788s 30s/step - loss: 0.0566 - accuracy: 1.0000 - val_loss: 0.0822 - val_accuracy: 0.9875\n",
            "Epoch 5/5\n",
            "60/60 [==============================] - 1768s 29s/step - loss: 0.0413 - accuracy: 1.0000 - val_loss: 0.0732 - val_accuracy: 0.9875\n",
            "\n",
            "Training History (Optimized):\n",
            "{'loss': [0.25416648387908936, 0.13200989365577698, 0.08231466263532639, 0.05662664398550987, 0.041300274431705475], 'accuracy': [0.9947916865348816, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.1974584013223648, 0.1361314207315445, 0.09995925426483154, 0.08217423409223557, 0.07323849946260452], 'val_accuracy': [0.987500011920929, 0.9833333492279053, 0.987500011920929, 0.987500011920929, 0.987500011920929]}\n",
            "15/15 [==============================] - 134s 9s/step - loss: 0.0732 - accuracy: 0.9875\n",
            "\n",
            "Test Loss (Optimized): 0.0732\n",
            "Test Accuracy (Optimized): 0.9875\n",
            "15/15 [==============================] - 137s 9s/step\n",
            "\n",
            "Classification Report (Optimized):\n",
            "                                 precision    recall  f1-score   support\n",
            "\n",
            "                           Acne       1.00      1.00      1.00         7\n",
            "                      Arthritis       1.00      1.00      1.00        10\n",
            "               Bronchial Asthma       1.00      1.00      1.00        11\n",
            "           Cervical spondylosis       1.00      1.00      1.00         7\n",
            "                    Chicken pox       1.00      0.92      0.96        12\n",
            "                    Common Cold       1.00      1.00      1.00        12\n",
            "                         Dengue       0.86      1.00      0.92        12\n",
            "          Dimorphic Hemorrhoids       1.00      1.00      1.00         7\n",
            "               Fungal infection       1.00      1.00      1.00        13\n",
            "                   Hypertension       1.00      1.00      1.00        10\n",
            "                       Impetigo       1.00      1.00      1.00        11\n",
            "                       Jaundice       1.00      1.00      1.00        11\n",
            "                        Malaria       1.00      1.00      1.00        11\n",
            "                       Migraine       1.00      1.00      1.00        10\n",
            "                      Pneumonia       1.00      1.00      1.00        11\n",
            "                      Psoriasis       1.00      0.83      0.91         6\n",
            "                        Typhoid       1.00      1.00      1.00         9\n",
            "                 Varicose Veins       1.00      1.00      1.00        12\n",
            "                        allergy       1.00      1.00      1.00        12\n",
            "                       diabetes       1.00      1.00      1.00         8\n",
            "                  drug reaction       1.00      1.00      1.00         5\n",
            "gastroesophageal reflux disease       1.00      0.91      0.95        11\n",
            "           peptic ulcer disease       0.92      1.00      0.96        11\n",
            "        urinary tract infection       1.00      1.00      1.00        11\n",
            "\n",
            "                       accuracy                           0.99       240\n",
            "                      macro avg       0.99      0.99      0.99       240\n",
            "                   weighted avg       0.99      0.99      0.99       240\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# prompt: optimization on model\n",
        "\n",
        "# Fine-tuning with a smaller learning rate and more epochs (example)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5) # Smaller learning rate\n",
        "# Use the original loss function object instead of the numerical value\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[metric])\n",
        "\n",
        "epochs = 5  # Increased epochs\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=epochs,\n",
        "    validation_data=test_dataset\n",
        ")\n",
        "\n",
        "print(\"\\nTraining History (Optimized):\")\n",
        "print(history.history)\n",
        "\n",
        "# Evaluate the model on the test set after further training\n",
        "loss_optimized, accuracy_optimized = model.evaluate(test_dataset)\n",
        "print(f\"\\nTest Loss (Optimized): {loss_optimized:.4f}\")\n",
        "print(f\"Test Accuracy (Optimized): {accuracy_optimized:.4f}\")\n",
        "\n",
        "# Predict and report again\n",
        "predictions_optimized = model.predict(test_dataset)\n",
        "predicted_labels_optimized = tf.argmax(predictions_optimized.logits, axis=1).numpy()\n",
        "predicted_diagnoses_optimized = [label_to_diagnosis[label] for label in predicted_labels_optimized]\n",
        "\n",
        "print(\"\\nClassification Report (Optimized):\")\n",
        "print(classification_report(true_diagnoses, predicted_diagnoses_optimized))\n",
        "\n",
        "# You can explore other optimizations like:\n",
        "# - Adjusting max_length\n",
        "# - Trying different batch sizes\n",
        "# - Implementing early stopping\n",
        "# - Using learning rate schedules\n",
        "# - Exploring different pre-trained BERT models (e.g., `bert-large-uncased`)\n",
        "# - Data augmentation (if applicable to text data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSPPJeHDYm9E"
      },
      "source": [
        "\n",
        "**Explanation:**\n",
        "\n",
        "1.  **Setup and Imports:**\n",
        "    *   Installs necessary libraries: `transformers` for BERT, `tensorflow` and `keras` for building and training the model, and `scikit-learn` for data splitting and evaluation.\n",
        "    *   Imports the required modules.\n",
        "    *   Prints the TensorFlow version for verification.\n",
        "\n",
        "2.  **Mock Data Generation:**\n",
        "    *   Creates a simple pandas DataFrame (`df`) to simulate a dataset of patient symptoms and their corresponding diagnoses. **In a real-world scenario, you would load your actual healthcare dataset here.**\n",
        "    *   Maps the text diagnoses to numerical labels for model training.\n",
        "    *   Prints a sample of the data and the diagnosis-to-label mapping.\n",
        "\n",
        "3.  **Data Preprocessing:**\n",
        "    *   Splits the data into training and testing sets using `train_test_split`.\n",
        "    *   Loads the `BertTokenizer` from the pre-trained `bert-base-uncased` model. The tokenizer is crucial for converting text into a format that BERT understands (token IDs, attention masks, and token type IDs).\n",
        "    *   Defines the `encode_text` function to tokenize and pad/truncate the symptom text to a fixed `max_length`.\n",
        "    *   Encodes the training and testing text data.\n",
        "    *   Creates TensorFlow `Dataset` objects from the encoded data and labels. This is a more efficient way to handle data during training in TensorFlow.\n",
        "    *   Batches and shuffles the training dataset for better training performance.\n",
        "\n",
        "4.  **Model Building:**\n",
        "    *   Loads the `TFBertForSequenceClassification` model from the pre-trained `bert-base-uncased` weights. This model is specifically designed for classification tasks using BERT. `num_labels` is set to the number of unique diagnoses.\n",
        "    *   Compiles the model using the Adam optimizer, Sparse Categorical Crossentropy loss (suitable for multi-class classification with integer labels), and Sparse Categorical Accuracy as the evaluation metric.\n",
        "    *   Prints the model summary to see its architecture.\n",
        "\n",
        "5.  **Model Training:**\n",
        "    *   Trains the model using the `fit` method, providing the training dataset and the number of `epochs`.\n",
        "    *   Includes `validation_data` (the test dataset) to monitor the model's performance on unseen data during training.\n",
        "    *   Prints the training history, which includes loss and accuracy for both training and validation sets over epochs.\n",
        "\n",
        "6.  **Model Evaluation:**\n",
        "    *   Evaluates the trained model on the test dataset using the `evaluate` method to get the final loss and accuracy on the test set.\n",
        "    *   Uses the `predict` method to get predictions for the test set.\n",
        "    *   Converts the predicted numerical labels back to their corresponding diagnoses using the `label_to_diagnosis` mapping.\n",
        "    *   Prints a `classification_report` which provides detailed metrics like precision, recall, F1-score, and support for each diagnosis class.\n",
        "\n",
        "7.  **Using the Health AI Agent for Prediction:**\n",
        "    *   Defines the `predict_diagnosis` function to take a symptom text as input and return the predicted diagnosis using the trained model and tokenizer.\n",
        "    *   Demonstrates how to use the function with new symptom examples.\n",
        "\n",
        "8.  **Saving and Loading the Model (Optional):**\n",
        "    *   Includes commented-out code to show how to save and load the trained model's weights and configuration. This is useful for deploying the model later without retraining.\n",
        "\n",
        "**To use this code effectively:**\n",
        "\n",
        "1.  **Replace Mock Data:** The most crucial step is to replace the mock `df` with your actual healthcare dataset. This dataset should contain symptom descriptions and their corresponding diagnoses. The quality and quantity of your data will significantly impact the agent's performance.\n",
        "2.  **Data Cleaning and Preprocessing:** Real-world healthcare data often requires extensive cleaning and preprocessing. This might involve handling missing values, standardizing text, removing noise, etc.\n",
        "3.  **Hyperparameter Tuning:** Experiment with different `max_length`, `batch_size`, `epochs`, and optimizer learning rates to find the best configuration for your dataset.\n",
        "4.  **Model Architecture:** While BERT is a powerful model, you might explore other transformer models or even simpler models depending on your data size and complexity.\n",
        "5.  **Domain Adaptation:** For better performance on medical text, consider using a BERT model pre-trained specifically on medical corpora (e.g., BioBERT, ClinicalBERT).\n",
        "6.  **Validation:** Implement more robust validation strategies (e.g., cross-validation) to ensure the model generalizes well.\n",
        "7.  **Deployment:** For production use, you would typically deploy the trained model using frameworks like TensorFlow Serving or cloud platforms.\n",
        "8.  **Ethical Considerations:** Be mindful of the ethical implications of using AI in healthcare, including bias in data, transparency of predictions, and the importance of human oversight. This model is a simplified example and should not be used for real-world medical diagnoses without rigorous validation and regulatory approval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "024b5378"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Define a function to represent your dataset for the converter\n",
        "# This is a simplified representative dataset. For better quantization,\n",
        "# use a more diverse and representative subset of your training data.\n",
        "def representative_dataset_gen():\n",
        "    for i in range(len(X_train)):\n",
        "        # Get a single training example\n",
        "        symptom = X_train.iloc[i:i+1].tolist()\n",
        "        # Preprocess the symptom text\n",
        "        encoding = encode_text(tokenizer, symptom, max_length=max_length)\n",
        "        # Yield the input tensors\n",
        "        yield [tf.constant(encoding['input_ids'], dtype=tf.int32),\n",
        "               tf.constant(encoding['token_type_ids'], dtype=tf.int32),\n",
        "               tf.constant(encoding['attention_mask'], dtype=tf.int32)]\n",
        "\n",
        "# Load the model (optional)\n",
        "loaded_model = TFBertForSequenceClassification.from_pretrained('./health_ai_agent_model')\n",
        "\n",
        "# Convert the Keras model to a TensorFlow Lite model with quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Specify the representative dataset for full integer quantization\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "\n",
        "# Ensure that ops that don't have a quantized implementation are allowed\n",
        "# to fall back to floating-point. This is often necessary for models\n",
        "# with complex operations.\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "\n",
        "# Convert the model\n",
        "tflite_model_quant = converter.convert()\n",
        "\n",
        "# Save the quantized model (optional)\n",
        "with open('health_ai_agent_quantized.tflite', 'wb') as f:\n",
        "    f.write(tflite_model_quant)\n",
        "\n",
        "print(\"Quantized TFLite model created and saved.\")\n",
        "\n",
        "# --- Evaluate the Quantized TFLite Model ---\n",
        "\n",
        "# Load the TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Prepare the test dataset for TFLite inference\n",
        "# Convert TensorFlow dataset to numpy arrays or lists\n",
        "test_input_ids = []\n",
        "test_token_type_ids = []\n",
        "test_attention_mask = []\n",
        "test_labels = []\n",
        "\n",
        "for batch in test_dataset:\n",
        "    inputs, labels = batch\n",
        "    test_input_ids.extend(inputs['input_ids'].numpy())\n",
        "    test_token_type_ids.extend(inputs['token_type_ids'].numpy())\n",
        "    test_attention_mask.extend(inputs['attention_mask'].numpy())\n",
        "    test_labels.extend(labels.numpy())\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "test_input_ids = np.array(test_input_ids, dtype=np.int32)\n",
        "test_token_type_ids = np.array(test_token_type_ids, dtype=np.int32)\n",
        "test_attention_mask = np.array(test_attention_mask, dtype=np.int32)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "\n",
        "# Run inference on the TFLite model\n",
        "tflite_predictions = []\n",
        "for i in range(len(test_input_ids)):\n",
        "    # Set the input tensor\n",
        "    interpreter.set_tensor(input_details[0]['index'], np.array([test_input_ids[i]]))\n",
        "    interpreter.set_tensor(input_details[1]['index'], np.array([test_token_type_ids[i]]))\n",
        "    interpreter.set_tensor(input_details[2]['index'], np.array([test_attention_mask[i]]))\n",
        "\n",
        "    # Run inference\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Get the output tensor and append to results\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "    tflite_predictions.append(output_data[0]) # Get the logits\n",
        "\n",
        "# Convert TFLite predictions (logits) to predicted labels\n",
        "tflite_predicted_labels = np.argmax(tflite_predictions, axis=1)\n",
        "\n",
        "# Convert numerical labels back to diagnoses\n",
        "label_to_diagnosis = {i: diagnosis for diagnosis, i in diagnosis_to_label.items()}\n",
        "tflite_predicted_diagnoses = [label_to_diagnosis[label] for label in tflite_predicted_labels]\n",
        "\n",
        "# Print classification report for the quantized model\n",
        "print(\"\\nClassification Report (Quantized TFLite Model):\")\n",
        "print(classification_report(test_labels, tflite_predicted_diagnoses, target_names=unique_diagnoses, labels=list(diagnosis_to_label.values())))\n",
        "\n",
        "# Compare model sizes\n",
        "# Ensure the directory exists if saving the Keras model\n",
        "keras_model_dir = './health_ai_agent_model'\n",
        "if not os.path.exists(keras_model_dir):\n",
        "    # Save the Keras model first if it hasn't been saved\n",
        "    try:\n",
        "        model.save_pretrained(keras_model_dir)\n",
        "        print(f\"Keras model saved to {keras_model_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not save Keras model: {e}\")\n",
        "\n",
        "\n",
        "original_model_size = 0\n",
        "for root, dirs, files in os.walk(keras_model_dir):\n",
        "    for file in files:\n",
        "        original_model_size += os.path.getsize(os.path.join(root, file))\n",
        "\n",
        "\n",
        "quantized_model_path = 'health_ai_agent_quantized.tflite'\n",
        "quantized_model_size = os.path.getsize(quantized_model_path) if os.path.exists(quantized_model_path) else 0\n",
        "\n",
        "\n",
        "print(f\"\\nOriginal Keras Model Size (approx): {original_model_size / 1024**2:.2f} MB\")\n",
        "print(f\"Quantized TFLite Model Size: {quantized_model_size / 1024**2:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "777fcfd5"
      },
      "source": [
        "### Explanation of Model Quantization Steps\n",
        "\n",
        "Model quantization is essentially a technique to reduce the size of your trained model and make it run faster, especially on devices with limited resources. It does this by reducing the precision of the numbers (weights and activations) in the model, often from 32-bit floating-point numbers to 8-bit integers.\n",
        "\n",
        "Here are the steps taken in the code cell above to apply post-training integer quantization:\n",
        "\n",
        "1.  **Defining a Representative Dataset (`representative_dataset_gen` function):**\n",
        "    *   **Purpose:** For **full integer quantization** (quantizing both weights and activations), the TensorFlow Lite converter needs a small, representative sample of your *training* data. It uses this data to determine the range (minimum and maximum values) for the activations in each layer of the model. This range is then used to map the floating-point values to fixed-point integers during quantization.\n",
        "    *   **Implementation:** The `representative_dataset_gen` function is a generator that iterates through your training data (`X_train`), preprocesses each symptom text using the same tokenizer as before, and yields the input tensors (`input_ids`, `token_type_ids`, `attention_mask`) in the format expected by the converter.\n",
        "\n",
        "2.  **Creating a TFLiteConverter (`tf.lite.TFLiteConverter.from_keras_model`):**\n",
        "    *   **Purpose:** This object is the core tool for converting your trained TensorFlow Keras model into the TensorFlow Lite format (`.tflite`).\n",
        "    *   **Implementation:** We create a converter instance and pass our trained Keras `model` to it.\n",
        "\n",
        "3.  **Setting Optimization Options (`converter.optimizations = [tf.lite.Optimize.DEFAULT]`):**\n",
        "    *   **Purpose:** This line tells the converter to apply default optimizations during conversion. This includes quantization. `tf.lite.Optimize.DEFAULT` currently enables post-training quantization.\n",
        "    *   **Implementation:** We set the `optimizations` attribute of the converter.\n",
        "\n",
        "4.  **Specifying the Representative Dataset (`converter.representative_dataset = representative_dataset_gen`):**\n",
        "    *   **Purpose:** This is where we provide the converter with the representative dataset generator we defined earlier. This is necessary for the converter to perform full integer quantization.\n",
        "    *   **Implementation:** We assign our `representative_dataset_gen` function to the `representative_dataset` attribute.\n",
        "\n",
        "5.  **Setting Target Specification (`converter.target_spec.supported_ops = [...]`):**\n",
        "    *   **Purpose:** This line is important for handling operations in your model that might not have a direct integer implementation in TensorFlow Lite. It tells the converter that it's okay to \"fall back\" to using floating-point operations for those specific parts of the model. This prevents conversion errors but results in a hybrid model (partially integer, partially floating-point).\n",
        "    *   **Implementation:** We set the `supported_ops` attribute to include both `TFLITE_BUILTINS_INT8` (for integer operations) and `TFLITE_BUILTINS` (for floating-point operations).\n",
        "\n",
        "6.  **Converting the Model (`tflite_model_quant = converter.convert()`):**\n",
        "    *   **Purpose:** This is the step where the actual conversion and quantization happen. The converter uses the model, optimization settings, and representative dataset to produce the quantized TFLite model.\n",
        "    *   **Implementation:** We call the `convert()` method on the converter object.\n",
        "\n",
        "7.  **Saving the Quantized Model:**\n",
        "    *   **Purpose:** To save the resulting quantized TFLite model to a file (`.tflite`) so you can load and use it later without reconverting.\n",
        "    *   **Implementation:** The converted model is a byte string, which is written to a file named `health_ai_agent_quantized.tflite`.\n",
        "\n",
        "8.  **Evaluating the Quantized Model:**\n",
        "    *   **Purpose:** To see how the quantization affected the model's performance (accuracy). Quantization can sometimes lead to a small drop in accuracy.\n",
        "    *   **Implementation:**\n",
        "        *   A `tf.lite.Interpreter` is created to load and run the TFLite model.\n",
        "        *   Input and output details of the TFLite model are retrieved.\n",
        "        *   The test dataset, which was originally in TensorFlow Dataset format, is converted into NumPy arrays because the TFLite interpreter works with NumPy arrays.\n",
        "        *   Inference is run sample by sample: the input tensors for each test example are set, the interpreter is invoked, and the output (logits) is retrieved.\n",
        "        *   The collected logits are converted back to predicted labels and then to diagnoses.\n",
        "        *   Finally, a classification report is printed for the quantized model, and its size is compared to the original model size.\n",
        "\n",
        "This process allows you to reduce the model size and potentially improve inference speed, which is particularly useful for deployment on devices with limited resources."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}